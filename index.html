<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" 
        content="We present a novel audio-visual Occlusion-Robust Gender Recognition and Age Estimation (ORAGEN) approach. The proposed approach is based on intermediate features of unimodal transformer-based models and two Multi-Task Cross-Modal Attention (MTCMA) blocks, which predict gender, age, and protective mask type using voice and facial characteristics.">
    <meta name="keywords" content="TRIPONES, Affective States Recognition, Emotion Recognition, Sentiment Recognition, Multi-Modal Fusion, Deep Learning, Transformer, Mamba, xLSTM">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Multi-Lingual Approach for Multi-Modal Emotion and Sentiment Recognition Based on Triple Fusion</title> 

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <link rel="icon" type="image/png" href="https://hci.nw.ru/img/favicon/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="https://hci.nw.ru/img/favicon/favicon-196x196.png" sizes="196x196">
    <link rel="icon" type="image/png" href="https://hci.nw.ru/img/favicon/favicon-228x228.png" sizes="228x228">
    <link rel="shortcut icon" href="https://hci.nw.ru/img/favicon/favicon.ico" type="image/x-icon">

    <link rel="apple-touch-icon" href="https://hci.nw.ru/img/favicon/apple-touch-icon-57x57.png" sizes="52x57">
    <link rel="apple-touch-icon" href="https://hci.nw.ru/img/favicon/apple-touch-icon-60x60.png" sizes="60x60">
    <link rel="apple-touch-icon" href="https://hci.nw.ru/img/favicon/apple-touch-icon-72x72.png" sizes="72x72">
    <link rel="apple-touch-icon" href="https://hci.nw.ru/img/favicon/apple-touch-icon-76x76.png" sizes="76x76">
    <link rel="apple-touch-icon" href="https://hci.nw.ru/img/favicon/apple-touch-icon-114x114.png" sizes="114x114">
    <link rel="apple-touch-icon" href="https://hci.nw.ru/img/favicon/apple-touch-icon-120x120.png" sizes="120x120">
    <link rel="apple-touch-icon" href="https://hci.nw.ru/img/favicon/apple-touch-icon-144x144.png" sizes="144x144">
    <link rel="apple-touch-icon" href="https://hci.nw.ru/img/favicon/apple-touch-icon-152x152.png" sizes="152x152">
    <link rel="apple-touch-icon" href="https://hci.nw.ru/img/favicon/apple-touch-icon-180x180.png" sizes="180x180">
    <link href="https://fonts.googleapis.com/css?family=Merriweather:400,900,900i" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/comp-slider.js" defer></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>
<body>
    <a id="btt-button">
        <svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 24 24"> <polygon id="btt-arr" fill="#BBB" points="12 6.586 3.293 15.293 4.707 16.707 12 9.414 19.293 16.707 20.707 15.293 12 6.586"/>
        </svg>
    </a>

    <section class="hero">
        <div class="hero-body sepline">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            Multi-Lingual Approach for Multi-Modal Emotion and Sentiment Recognition Based on Triple Fusion
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://hci.nw.ru/en/employees/10" target="_blank">Maxim Markitantov</a><sup>1,*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://hci.nw.ru/en/employees/14" target="_blank">Elena Ryumina</a><sup>1,*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://hci.nw.ru/en/employees/13" target="_blank">Anastasia Dvoynikova</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://hci.nw.ru/en/employees/1" target="_blank">Alexey Karpov</a><sup>1,2</sup>,
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <sup>1</sup> St. Petersburg Institute for Informatics and Automation of the Russian Academy of Sciences, <a href="https://spcras.ru/en/" target="_blank">St. Petersburg Federal Research Center of the Russian Academy of Sciences (SPC RAS)</a>, St. Petersburg, Russia
                            </span>
                            <br />
                            <span class="author-block">
                                <sup>2</sup> <a href="https://en.itmo.ru/" target="_blank">ITMO University</a>, St. Petersburg, Russia
                            </span>
                            <br />
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->

                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/markitantov/TRIFONES" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                
                                <!-- Models Link. -->
                                <span class="link-block">
                                    <a href="#" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span> 
                                        <span>Models (coming soon)</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section sepline">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <div class="content">
                        <h3 class="title is-3">Abstract</h3>
                        <p class="has-text-justified">
                            Affective states recognition is a challenging task that requires a large amount of input data, such as audio, video, and text. Current multi-modal approaches are often single-task and corpus-specific, resulting in overfitting, poor generalization across corpora, and reduced real-world performance. In this work, we address these limitations by: (1) multi-lingual training on corpora that include Russian (RAMAS) and English (MELD, CMU-MOSEI) speech; (2) multi-task learning for joint emotion and sentiment recognition; and (3) a novel Triple Fusion strategy that employs cross-modal integration at both hierarchical unimodal and fused multi-modal feature levels, enhancing intra- and inter-modal relationships of different affective states and modalities. Additionally, to optimize performance of the approach proposed, we compare temporal encoders (Transformer-based, Mamba, xLSTM) and fusion strategies (double and triple fusion strategies with and without a label encoder) to comprehensively understand their capabilities and limitations. On the Test subset of the CMU-MOSEI corpus, the proposed approach showed mean weighted F1-score (mWF) of 88.6%, and weighted F1-score (WF) of 84.8% for emotion and sentiment recognition, respectively. On the Test subset of the MELD corpus, the proposed approach showed WF of 49.6% and WF of 60.0%, respectively. On the Test subset of the RAMAS corpus, the proposed approach showed WF of 71.8% and WF of 90.0%, respectively. We compare the performance of the approach proposed with that of the SOTA ones.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer" style="background-color: rgb(247, 247, 247);">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                <div class="content">
                    <p>
                    This page was built using the <a href="https://github.com/SMIL-SPCRAS/ORAGEN" target="_blank">ORAGEN page repository</a>, which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                    You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                </div>
                </div>
            </div>
        </div>
    </footer>
</body>
</html>
